{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cbef579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 CSV files in ckp/gmm_ckp/resnet18_on_tinyimagenet\n",
      "Function 'inspect_csv_columns' defined successfully!\n",
      "Functions defined: load_gmm_config(), load_gmm_model()\n",
      "ckp/gmm_ckp/resnet18_on_tinyimagenet/loss_hist_K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).csv\n",
      "ckp/gmm_ckp/resnet18_on_tinyimagenet/collapse_log_K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).csv\n",
      "ckp/gmm_ckp/resnet18_on_tinyimagenet/gmm_K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "import os\n",
    "import torch\n",
    "from utils import GMM4PR, get_dataset, build_model, build_decoder_from_flag\n",
    "from typing import Tuple, Dict, Any\n",
    "# Define CSV files to analyze\n",
    "# Get all CSV files from the directory using pathlib\n",
    "\n",
    "# csv_dir = Path('./ckp/gmm_ckp/resnet18_on_cifar10')\n",
    "# csv_dir = Path('./ckp/gmm_ckp/resnet50_on_cifar10')\n",
    "csv_dir = Path('./ckp/gmm_ckp/resnet18_on_tinyimagenet')\n",
    "# csv_dir = Path('./ckp/gmm_ckp/vgg16_on_cifar10')\n",
    "\n",
    "all_csv_files = sorted([str(f) for f in csv_dir.glob('*.csv')])\n",
    "\n",
    "print(f\"Found {len(all_csv_files)} CSV files in {csv_dir}\")\n",
    "\n",
    "# Inspect first few files\n",
    "def inspect_csv_columns(csv_files: List[str], last_row_idx: int = -1):\n",
    "    \"\"\"\n",
    "    Inspect and display columns available in CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_files : List[str]\n",
    "        List of CSV file paths to inspect\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"CSV File Column Inspection\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"\\nFile: {Path(csv_file).name}\")\n",
    "        print(f\"  Shape: {df.shape} (rows, columns)\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print(f\"  Last few rows:\")\n",
    "        print(df.iloc[:last_row_idx].tail(3).to_string(index=False))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "print(\"Function 'inspect_csv_columns' defined successfully!\")\n",
    "\n",
    "\n",
    "def load_gmm_config(gmm_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the configuration from a saved GMM checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        gmm_path: Path to the GMM checkpoint file (.pt)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing both GMM config and full experiment config\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(gmm_path):\n",
    "        raise FileNotFoundError(f\"GMM checkpoint not found: {gmm_path}\")\n",
    "    \n",
    "    ckpt = torch.load(gmm_path, map_location=\"cpu\", weights_only=False)\n",
    "    \n",
    "    if \"config\" not in ckpt:\n",
    "        raise ValueError(f\"No config found in checkpoint: {gmm_path}\")\n",
    "    \n",
    "    gmm_config = ckpt[\"config\"]\n",
    "    \n",
    "    # The full experiment config is stored inside gmm_config[\"config\"]\n",
    "    exp_config = gmm_config.get(\"config\", {})\n",
    "    \n",
    "    print(f\"✓ Loaded config from: {gmm_path}\")\n",
    "    print(f\"  Experiment: {exp_config.get('exp_name', 'N/A')}\")\n",
    "    print(f\"  Dataset: {exp_config.get('dataset', 'N/A')}\")\n",
    "    print(f\"  Architecture: {exp_config.get('arch', 'N/A')}\")\n",
    "    print(f\"  K: {gmm_config['K']}, latent_dim: {gmm_config['latent_dim']}\")\n",
    "    print(f\"  Condition mode: {gmm_config.get('cond_mode', 'None')}\")\n",
    "    print(f\"  Covariance type: {gmm_config.get('cov_type', 'diag')}\")\n",
    "    print(f\"  Perturbation: {gmm_config.get('budget', {})}\")\n",
    "    \n",
    "    return gmm_config\n",
    "\n",
    "\n",
    "def load_gmm_model(gmm_path: str, device: str = \"cuda\") -> Tuple[GMM4PR, Any, Any, Any]:\n",
    "    \"\"\"\n",
    "    Load a complete GMM model from checkpoint with all necessary components.\n",
    "    \n",
    "    Args:\n",
    "        gmm_path: Path to the GMM checkpoint file (.pt)\n",
    "        device: Device to load the model to ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (gmm_model, classifier_model, feat_extractor, up_sampler)\n",
    "    \"\"\"\n",
    "    # Step 1: Load configuration\n",
    "    gmm_config = load_gmm_config(gmm_path)\n",
    "    exp_config = gmm_config.get(\"config\", {})\n",
    "    \n",
    "    # Extract necessary config values\n",
    "    dataset_name = exp_config.get(\"dataset\", \"cifar10\")\n",
    "    data_root = exp_config.get(\"data_root\", \"./dataset\")\n",
    "    arch = exp_config.get(\"arch\", \"resnet18\")\n",
    "    clf_ckpt = exp_config.get(\"clf_ckpt\")\n",
    "    use_decoder = exp_config.get(\"use_decoder\", False)\n",
    "    decoder_backend = exp_config.get(\"decoder_backend\", \"bicubic\")\n",
    "    latent_dim = gmm_config[\"latent_dim\"]\n",
    "    \n",
    "    # Step 2: Load dataset to get output shape\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset, num_classes, out_shape = get_dataset(dataset_name, data_root, train=True, resize=False)\n",
    "    print(f\"  Dataset: {dataset_name}, Classes: {num_classes}, Shape: {out_shape}\")\n",
    "    \n",
    "    # Step 3: Load classifier and feature extractor\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading classifier: {arch}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    if not clf_ckpt or not os.path.isfile(clf_ckpt):\n",
    "        raise FileNotFoundError(f\"Classifier checkpoint not found: {clf_ckpt}\")\n",
    "    \n",
    "    model, feat_extractor = build_model(arch, num_classes, device)\n",
    "    \n",
    "    # Load classifier weights\n",
    "    state = torch.load(clf_ckpt, map_location=\"cpu\", weights_only=False)\n",
    "    state = state.get(\"state_dict\", state.get(\"model_state\", state))\n",
    "    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
    "    \n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model = model.to(device).eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    feat_extractor = feat_extractor.to(device).eval()\n",
    "    for p in feat_extractor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Check parameter sharing\n",
    "    model_params = {id(p) for p in model.parameters()}\n",
    "    feat_params = {id(p) for p in feat_extractor.parameters()}\n",
    "    shared = model_params & feat_params\n",
    "    \n",
    "    print(f\"  Model params: {len(model_params)}, Feat extractor params: {len(feat_params)}\")\n",
    "    if shared:\n",
    "        print(f\"  Shared parameters: {len(shared)}\")\n",
    "    \n",
    "    # Step 4: Build decoder/up_sampler if needed\n",
    "    up_sampler = None\n",
    "    if use_decoder:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Building decoder: {decoder_backend}\")\n",
    "        up_sampler = build_decoder_from_flag(\n",
    "            decoder_backend,\n",
    "            latent_dim,\n",
    "            out_shape,\n",
    "            device\n",
    "        )\n",
    "        print(f\"  ✓ Decoder built successfully!\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"No decoder used (use_decoder={use_decoder})\")\n",
    "    \n",
    "    # Step 5: Load the GMM model\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading GMM model from: {gmm_path}\")\n",
    "    gmm = GMM4PR.load_from_checkpoint(\n",
    "        gmm_path,\n",
    "        feat_extractor=feat_extractor,\n",
    "        up_sampler=up_sampler,\n",
    "        map_location=device,\n",
    "        strict=True\n",
    "    )\n",
    "    \n",
    "    gmm = gmm.to(device).eval()\n",
    "    print(f\"✓ GMM model loaded successfully!\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return gmm, gmm_config, model, feat_extractor, up_sampler\n",
    "\n",
    "\n",
    "print(\"Functions defined: load_gmm_config(), load_gmm_model()\")\n",
    "\n",
    "\n",
    "keywords = ['loss_hist', 'cond(xy)', 'K7', 'decoder(trainable_128)', 'linf(16)' ,'reg(none)']\n",
    "keywords_entropy = keywords.copy()\n",
    "keywords_entropy[0] = 'collapse_log'\n",
    "filtered_csv_files = [f for f in all_csv_files if all(k in f for k in keywords)]\n",
    "filtered_csv_files_entropy = [f for f in all_csv_files if all(k in f for k in keywords_entropy)]\n",
    "\n",
    "# Create corresponding GMM model paths from the filtered CSV files\n",
    "gmm_model_paths = [f.replace('loss_hist_', 'gmm_').replace('.csv', '.pt') for f in filtered_csv_files]\n",
    "print(\"\\n\".join([f\"{n}\" for n in filtered_csv_files]))\n",
    "print(\"\\n\".join([f\"{n}\" for n in filtered_csv_files_entropy]))\n",
    "print(\"\\n\".join([f\"{n}\" for n in gmm_model_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e9465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded config from: ckp/gmm_ckp/resnet18_on_tinyimagenet/gmm_K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).pt\n",
      "  Experiment: K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none)\n",
      "  Dataset: tinyimagenet\n",
      "  Architecture: resnet18\n",
      "  K: 7, latent_dim: 128\n",
      "  Condition mode: xy\n",
      "  Covariance type: full\n",
      "  Perturbation: {'norm': 'linf', 'eps': 0.06274509803921569}\n",
      "\n",
      "============================================================\n",
      "Loading dataset...\n",
      "  Dataset: tinyimagenet, Classes: 200, Shape: (3, 64, 64)\n",
      "\n",
      "============================================================\n",
      "Loading classifier: resnet18\n",
      "  Device: cuda\n",
      "  Model params: 62, Feat extractor params: 60\n",
      "  Shared parameters: 60\n",
      "\n",
      "============================================================\n",
      "Building decoder: bicubic_trainable\n",
      "[Decoder 'bicubic'] 18,963 params\n",
      "  ✓ Decoder built successfully!\n",
      "\n",
      "============================================================\n",
      "Loading GMM model from: ckp/gmm_ckp/resnet18_on_tinyimagenet/gmm_K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang3_z@WMGDS.WMG.WARWICK.AC.UK/WSPR/utils/gmm4pr.py:992: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Params] Shared trunk: 263,680 | pi: 903 | mu: 459,648, | cov: 59,124,224 | Total: 59,584,775\n",
      "Model loaded from ckp/gmm_ckp/resnet18_on_tinyimagenet/gmm_K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).pt\n",
      "✓ GMM model loaded successfully!\n",
      "============================================================\n",
      "\n",
      "\n",
      "Computing PR on clean-correct samples...\n",
      "  GMM: K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none)\n",
      "  Samples per image: 500\n",
      "  Batch indices: range(0, 1000)\n",
      "[PR@clean] used=5068 / seen=10000 (clean acc=50.68%), num_samples=500, chunk_size=32 → PR=0.9296 [method: soft (Gumbel-Softmax)]\n",
      "\n",
      "============================================================\n",
      "Results:\n",
      "  Clean Accuracy: 50.68%\n",
      "  Probabilistic Robustness (PR): 0.9296479936462739\n",
      "  Clean-correct samples used: 5068\n",
      "============================================================\n",
      "\n",
      "All components loaded and ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_pr_on_clean_correct\n",
    "# Specify the GMM checkpoint path\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET = \"tinyimagenet\"\n",
    "\n",
    "res_collection = {}\n",
    "# Load the complete GMM model\n",
    "for GMM_PATH in gmm_model_paths:\n",
    "    gmm, cfg, model, feat_extractor, up_sampler = load_gmm_model(GMM_PATH, device=DEVICE)\n",
    "\n",
    "    test_dataset, num_classes, out_shape = get_dataset(DATASET, \"./dataset\", train=True, resize=False)\n",
    "    # Create DataLoader for evaluation\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Set number of samples per image\n",
    "    S_SAMPLES = 500  # Number of Monte Carlo samples per image\n",
    "\n",
    "    # Optional: Specify batch indices to evaluate (None = all batches)\n",
    "    # BATCH_INDICES = None  # Evaluate all batches\n",
    "    # BATCH_INDICES = [0, 1, 2, 3, 4]  # Evaluate only first 5 batches\n",
    "    BATCH_INDICES = range(1000) \n",
    "\n",
    "    print(f\"\\nComputing PR on clean-correct samples...\")\n",
    "    print(f\"  GMM: {cfg['config']['exp_name']}\")\n",
    "    print(f\"  Samples per image: {S_SAMPLES}\")\n",
    "    print(f\"  Batch indices: {'All' if BATCH_INDICES is None else BATCH_INDICES}\")\n",
    "\n",
    "    # Compute PR\n",
    "    pr, n_used, clean_acc = compute_pr_on_clean_correct(\n",
    "        model=model,\n",
    "        gmm=gmm,\n",
    "        loader=loader,\n",
    "        out_shape=out_shape,\n",
    "        device=DEVICE,\n",
    "        num_samples=S_SAMPLES,\n",
    "        batch_indices=BATCH_INDICES,\n",
    "        temperature=1,  # Very low temperature should approximate hard sampling\n",
    "        use_soft_sampling=True,\n",
    "        chunk_size=32\n",
    "    )\n",
    "\n",
    "    res_collection[GMM_PATH] = pr\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results:\")\n",
    "    print(f\"  Clean Accuracy: {clean_acc*100:.2f}%\")\n",
    "    print(f\"  Probabilistic Robustness (PR): {pr}\")\n",
    "    print(f\"  Clean-correct samples used: {n_used}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(\"\\nAll components loaded and ready for analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685dd146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ckp/gmm_ckp/vgg16_on_tinyimagenet/gmm_K7_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).pt': 0.7883551615140205}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "490a43ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CSV File Column Inspection\n",
      "================================================================================\n",
      "\n",
      "File: loss_hist_K12_cond(xy)_decoder(none)_linf(16)_reg(none).csv\n",
      "  Shape: (50, 6) (rows, columns)\n",
      "  Columns: ['epoch', 'loss', 'main_loss', 'reg_loss', 'pr', 'learning_rate']\n",
      "  Last few rows:\n",
      " epoch     loss  main_loss  reg_loss       pr  learning_rate\n",
      "    48 4.566737   4.566737       0.0 0.761502         0.0005\n",
      "    49 4.560260   4.560260       0.0 0.760193         0.0005\n",
      "    50 4.554434   4.554434       0.0 0.759272         0.0005\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: loss_hist_K12_cond(xy)_decoder(nontrainable)_linf(16)_reg(none).csv\n",
      "  Shape: (50, 6) (rows, columns)\n",
      "  Columns: ['epoch', 'loss', 'main_loss', 'reg_loss', 'pr', 'learning_rate']\n",
      "  Last few rows:\n",
      " epoch     loss  main_loss  reg_loss       pr  learning_rate\n",
      "    48 5.481014   5.481014       0.0 0.893260         0.0005\n",
      "    49 5.477815   5.477815       0.0 0.892706         0.0005\n",
      "    50 5.475565   5.475565       0.0 0.892452         0.0005\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: loss_hist_K12_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).csv\n",
      "  Shape: (200, 6) (rows, columns)\n",
      "  Columns: ['epoch', 'loss', 'main_loss', 'reg_loss', 'pr', 'learning_rate']\n",
      "  Last few rows:\n",
      " epoch     loss  main_loss  reg_loss       pr  learning_rate\n",
      "   198 5.312306   5.312306       0.0 0.867608         0.0005\n",
      "   199 5.310649   5.310649       0.0 0.867175         0.0005\n",
      "   200 5.309708   5.309708       0.0 0.867137         0.0005\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inspect_csv_columns(filtered_csv_files, last_row_idx=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73b361e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CSV File Column Inspection\n",
      "================================================================================\n",
      "\n",
      "File: collapse_log_K12_cond(xy)_decoder(none)_linf(16)_reg(none).csv\n",
      "  Shape: (5, 8) (rows, columns)\n",
      "  Columns: ['epoch', 'max_pi', 'min_pi', 'std_pi', 'entropy_ratio', 'T_pi', 'T_gumbel', 'avg_loss']\n",
      "  Last few rows:\n",
      " epoch   max_pi   min_pi   std_pi  entropy_ratio  T_pi  T_gumbel  avg_loss\n",
      "    30 0.199573 0.000024 0.082800       0.761507   1.0  0.467347  4.728826\n",
      "    40 0.199597 0.000009 0.082816       0.761165   1.0  0.283673  4.633919\n",
      "    50 0.199605 0.000004 0.082822       0.761042   1.0  0.100000  4.554434\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: collapse_log_K12_cond(xy)_decoder(nontrainable)_linf(16)_reg(none).csv\n",
      "  Shape: (5, 8) (rows, columns)\n",
      "  Columns: ['epoch', 'max_pi', 'min_pi', 'std_pi', 'entropy_ratio', 'T_pi', 'T_gumbel', 'avg_loss']\n",
      "  Last few rows:\n",
      " epoch   max_pi   min_pi   std_pi  entropy_ratio  T_pi  T_gumbel  avg_loss\n",
      "    30 0.302273 0.000058 0.094034       0.738606   1.0  0.467347  5.545308\n",
      "    40 0.302324 0.000019 0.094070       0.737863   1.0  0.283673  5.506855\n",
      "    50 0.302335 0.000008 0.094080       0.737628   1.0  0.100000  5.475565\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "File: collapse_log_K12_cond(xy)_decoder(trainable_128)_linf(16)_reg(none).csv\n",
      "  Shape: (20, 8) (rows, columns)\n",
      "  Columns: ['epoch', 'max_pi', 'min_pi', 'std_pi', 'entropy_ratio', 'T_pi', 'T_gumbel', 'avg_loss']\n",
      "  Last few rows:\n",
      " epoch   max_pi   min_pi   std_pi  entropy_ratio  T_pi  T_gumbel  avg_loss\n",
      "    30 0.305957 0.000151 0.094885       0.738102   1.8  0.868844  5.573414\n",
      "    40 0.306050 0.000140 0.094932       0.737617   1.4  0.823618  5.545866\n",
      "    50 0.306031 0.000191 0.094905       0.738250   1.0  0.778392  5.531351\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "inspect_csv_columns(filtered_csv_files_entropy, last_row_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vrwzq9yws6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'compute_pr_with_baseline_noise' defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def compute_pr_with_baseline_noise(\n",
    "    model,\n",
    "    loader,\n",
    "    out_shape,\n",
    "    device,\n",
    "    distribution='gaussian',\n",
    "    num_samples=500,\n",
    "    epsilon=16.0/255,\n",
    "    norm_type='linf',\n",
    "    batch_indices=None,\n",
    "    chunk_size=None,\n",
    "    clip_to_valid_range=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute PR on clean-correct set using baseline noise distributions (Gaussian or Uniform).\n",
    "    \n",
    "    This function samples perturbations directly in the input space from standard distributions\n",
    "    (no GMM, no decoder needed) and evaluates probabilistic robustness.\n",
    "    \n",
    "    PR = E_{(x,y) ∈ CleanCorrect} E_{delta~Noise} [ 1{ f(x+delta) = y } ]\n",
    "    \n",
    "    Args:\n",
    "        model: Classifier model (should be in eval mode)\n",
    "        loader: DataLoader for evaluation\n",
    "        out_shape: Image shape (C, H, W)\n",
    "        device: torch device\n",
    "        distribution: Type of noise distribution to use:\n",
    "                     - 'gaussian': Standard Gaussian N(0, I) in input space\n",
    "                     - 'uniform': Uniform distribution U(-1, 1) in input space\n",
    "        num_samples: Number of samples per image\n",
    "        epsilon: Perturbation budget (gamma parameter for g_ball)\n",
    "        norm_type: Type of norm constraint ('linf' or 'l2')\n",
    "        batch_indices: Optional set/list of batch indices to evaluate\n",
    "        chunk_size: Maximum samples to process at once. If None, process all at once.\n",
    "                   Useful for large num_samples to avoid OOM errors.\n",
    "        clip_to_valid_range: If True, clip perturbed images to [0, 1]. Default False to match \n",
    "                            GMM behavior (which doesn't clip).\n",
    "    \n",
    "    Returns:\n",
    "        (pr, n_used, clean_acc): PR score, number of clean-correct samples, clean accuracy\n",
    "    \"\"\"\n",
    "    from utils import g_ball\n",
    "    \n",
    "    C, H, W = out_shape\n",
    "    \n",
    "    total_used = 0       # number of clean-correct samples\n",
    "    pr_sum = 0.0         # sum of per-image PR values\n",
    "    clean_correct = 0    # number of correctly classified clean samples\n",
    "    total_seen = 0       # total samples seen\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for it, (x, y, _) in enumerate(loader):\n",
    "            if (batch_indices is not None) and (it not in batch_indices):\n",
    "                continue\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            B = x.size(0)\n",
    "            \n",
    "            # Clean predictions & mask of correct ones\n",
    "            logits_clean = model(x)\n",
    "            pred_clean = logits_clean.argmax(1)\n",
    "            mask = (pred_clean == y)\n",
    "            \n",
    "            clean_correct += mask.sum().item()\n",
    "            total_seen += B\n",
    "            \n",
    "            if mask.sum().item() == 0:\n",
    "                continue  # nothing to evaluate in this batch\n",
    "            \n",
    "            x_sel = x[mask]\n",
    "            y_sel = y[mask]\n",
    "            n = x_sel.size(0)\n",
    "            \n",
    "            # Determine chunk size for processing\n",
    "            if chunk_size is None:\n",
    "                # Process all samples at once\n",
    "                actual_chunk_size = num_samples\n",
    "            else:\n",
    "                actual_chunk_size = min(chunk_size, num_samples)\n",
    "            \n",
    "            # Accumulate success rate for each image\n",
    "            per_image_success = torch.zeros(n, device=device)\n",
    "            \n",
    "            # Process in chunks to avoid OOM\n",
    "            num_chunks = (num_samples + actual_chunk_size - 1) // actual_chunk_size\n",
    "            \n",
    "            for chunk_idx in range(num_chunks):\n",
    "                start_idx = chunk_idx * actual_chunk_size\n",
    "                end_idx = min((chunk_idx + 1) * actual_chunk_size, num_samples)\n",
    "                S_chunk = end_idx - start_idx\n",
    "                \n",
    "                # Sample noise from the specified distribution\n",
    "                # Shape: [S_chunk, n, C, H, W]\n",
    "                if distribution == 'gaussian':\n",
    "                    # Standard Gaussian: N(0, I)\n",
    "                    noise = torch.randn(S_chunk, n, C, H, W, device=device)\n",
    "                elif distribution == 'uniform':\n",
    "                    # Uniform: U(-1, 1)\n",
    "                    noise = torch.rand(S_chunk, n, C, H, W, device=device) * 2 - 1\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown distribution: {distribution}. Choose 'gaussian' or 'uniform'.\")\n",
    "                \n",
    "                # Reshape for g_ball: [S_chunk * n, C, H, W]\n",
    "                noise_flat = noise.view(S_chunk * n, C, H, W)\n",
    "                \n",
    "                # Apply g_ball to constrain to perturbation budget\n",
    "                perturbations = g_ball(noise_flat, gamma=epsilon, norm_type=norm_type)\n",
    "                \n",
    "                # Create perturbed images (matching GMM pipeline exactly)\n",
    "                x_rep = x_sel.unsqueeze(0).expand(S_chunk, -1, -1, -1, -1).reshape(S_chunk * n, C, H, W)\n",
    "                x_perturbed = x_rep + perturbations\n",
    "                \n",
    "                # Optionally clip to valid image range [0, 1]\n",
    "                # Note: GMM method does NOT clip by default, so we match that behavior\n",
    "                if clip_to_valid_range:\n",
    "                    x_perturbed = torch.clamp(x_perturbed, 0.0, 1.0)\n",
    "                \n",
    "                # Evaluate classifier on perturbed images\n",
    "                y_rep = y_sel.repeat(S_chunk)\n",
    "                logits = model(x_perturbed)\n",
    "                pred = logits.argmax(1)\n",
    "                \n",
    "                # Check which predictions match the true label\n",
    "                # Shape: [S_chunk * n] -> [S_chunk, n]\n",
    "                correct = (pred == y_rep).float().view(S_chunk, n)\n",
    "                \n",
    "                # Accumulate success rate\n",
    "                per_image_success += correct.sum(dim=0)\n",
    "            \n",
    "            # Compute per-image PR (average success rate over all samples)\n",
    "            per_image_pr = per_image_success / num_samples\n",
    "            \n",
    "            # Accumulate\n",
    "            pr_sum += per_image_pr.sum().item()\n",
    "            total_used += n\n",
    "    \n",
    "    pr = pr_sum / max(1, total_used)\n",
    "    clean_acc = clean_correct / max(1, total_seen)\n",
    "    \n",
    "    chunk_info = f\", chunk_size={chunk_size}\" if chunk_size is not None else \"\"\n",
    "    clip_info = \" (clipped)\" if clip_to_valid_range else \" (no clipping)\"\n",
    "    print(f\"[PR@clean - Baseline] used={total_used} / seen={total_seen} \"\n",
    "          f\"(clean acc={clean_acc*100:.2f}%), num_samples={num_samples}{chunk_info} → \"\n",
    "          f\"PR={pr:.4f} [distribution: {distribution}, norm: {norm_type}, eps={epsilon:.6f}{clip_info}]\")\n",
    "    \n",
    "    return pr, total_used, clean_acc\n",
    "\n",
    "\n",
    "print(\"Function 'compute_pr_with_baseline_noise' defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gfxos9kbxot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "================================================================================\n",
      "Evaluating with Standard Gaussian Noise\n",
      "================================================================================\n",
      "[PR@clean - Baseline] used=9029 / seen=10000 (clean acc=90.29%), num_samples=500, chunk_size=8 → PR=0.9919 [distribution: gaussian, norm: linf, eps=0.062745 (no clipping)]\n",
      "\n",
      "================================================================================\n",
      "Evaluating with Uniform Noise\n",
      "================================================================================\n",
      "[PR@clean - Baseline] used=9029 / seen=10000 (clean acc=90.29%), num_samples=500, chunk_size=32 → PR=0.9939 [distribution: uniform, norm: linf, eps=0.062745 (no clipping)]\n",
      "\n",
      "================================================================================\n",
      "Comparison Summary\n",
      "================================================================================\n",
      "dataset: cifar10, model: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      "), num_samples: 500, epsilon: 0.06274509803921569, norm: linf\n",
      "Gaussian PR: 0.9919\n",
      "Uniform PR:  0.9939\n",
      "GMM PR:      0.9638\n"
     ]
    }
   ],
   "source": [
    "# Example usage of compute_pr_with_baseline_noise\n",
    "# Uncomment and run to test the function\n",
    "\n",
    "# Load test dataset and model (reusing from above)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET = \"cifar10\"\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset, num_classes, out_shape = get_dataset(DATASET, \"./dataset\", train=False, resize=False)\n",
    "\n",
    "# Create DataLoader for evaluation\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Load a pretrained classifier (use the same one from GMM experiments)\n",
    "# model should be loaded already from previous cells\n",
    "\n",
    "# Set parameters\n",
    "NUM_SAMPLES = 500  # Number of samples per image\n",
    "EPSILON = 16 / 255  # Same as linf(16) in your GMM experiments\n",
    "NORM_TYPE = 'linf'  # 'linf' or 'l2'\n",
    "BATCH_INDICES = range(1000)  # Evaluate first 1000 batches, or None for all\n",
    "\n",
    "# Example 1: Evaluate with Gaussian noise\n",
    "print(\"=\"*80)\n",
    "print(\"Evaluating with Standard Gaussian Noise\")\n",
    "print(\"=\"*80)\n",
    "pr_gaussian, n_used_gaussian, clean_acc_gaussian = compute_pr_with_baseline_noise(\n",
    "    model=model,\n",
    "    loader=loader,\n",
    "    out_shape=out_shape,\n",
    "    device=DEVICE,\n",
    "    distribution='gaussian',\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    epsilon=EPSILON,\n",
    "    norm_type=NORM_TYPE,\n",
    "    batch_indices=BATCH_INDICES,\n",
    "    chunk_size=8  # Process 8 samples at a time to avoid OOM\n",
    ")\n",
    "\n",
    "# Example 2: Evaluate with Uniform noise\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating with Uniform Noise\")\n",
    "print(\"=\"*80)\n",
    "pr_uniform, n_used_uniform, clean_acc_uniform = compute_pr_with_baseline_noise(\n",
    "    model=model,\n",
    "    loader=loader,\n",
    "    out_shape=out_shape,\n",
    "    device=DEVICE,\n",
    "    distribution='uniform',\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    epsilon=EPSILON,\n",
    "    norm_type=NORM_TYPE,\n",
    "    batch_indices=BATCH_INDICES,\n",
    "    chunk_size=32\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"dataset: {DATASET}, model: {model}, num_samples: {NUM_SAMPLES}, epsilon: {EPSILON}, norm: {NORM_TYPE}\")\n",
    "print(f\"Gaussian PR: {pr_gaussian}\")\n",
    "print(f\"Uniform PR:  {pr_uniform}\")\n",
    "print(f\"GMM PR:      {res_collection[gmm_model_paths[0]]:.4f}\")  # From previous evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcee9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PGD and CW Attack Functions\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def accuracy(logits, target):\n",
    "    \"\"\"Compute accuracy given logits and target labels.\"\"\"\n",
    "    _, pred = torch.max(logits, dim=1)\n",
    "    correct = (pred == target).sum()\n",
    "    total = target.size(0)\n",
    "    acc = (float(correct) / total) * 100\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# PGD Attack\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def pgd_attack(model, x, y, epsilon=8/255, step_size=2/255, num_steps=10, random_init=True):\n",
    "    \"\"\"\n",
    "    Perform PGD (Projected Gradient Descent) attack on input images.\n",
    "    \n",
    "    Args:\n",
    "        model: Classifier model\n",
    "        x: Input images [B, C, H, W]\n",
    "        y: True labels [B]\n",
    "        epsilon: Maximum perturbation budget (L-inf norm)\n",
    "        step_size: Step size for each iteration\n",
    "        num_steps: Number of PGD iterations\n",
    "        random_init: Whether to initialize with random noise\n",
    "    \n",
    "    Returns:\n",
    "        x_adv: Adversarial examples [B, C, H, W]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x_adv = x.detach().clone()\n",
    "    \n",
    "    # Random initialization within epsilon ball\n",
    "    if random_init:\n",
    "        x_adv = x_adv + torch.empty_like(x_adv).uniform_(-epsilon, epsilon)\n",
    "        x_adv = torch.clamp(x_adv, min=0, max=1).detach()\n",
    "    \n",
    "    # PGD iterations\n",
    "    for _ in range(num_steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x_adv)\n",
    "        loss = F.cross_entropy(logits, y, reduction=\"mean\")\n",
    "        \n",
    "        # Backward pass\n",
    "        grad = torch.autograd.grad(loss, [x_adv])[0]\n",
    "        \n",
    "        # Update adversarial example\n",
    "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
    "        \n",
    "        # Project back to epsilon ball\n",
    "        delta = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)\n",
    "        x_adv = torch.clamp(x + delta, min=0, max=1).detach()\n",
    "    \n",
    "    return x_adv\n",
    "\n",
    "\n",
    "def evaluate_pgd(model, loader, device, epsilon=8/255, step_size=2/255, num_steps=10, \n",
    "                 batch_indices=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate model robustness under PGD attack on clean-correct samples.\n",
    "    \n",
    "    This function:\n",
    "    1. Identifies correctly classified samples (clean-correct set)\n",
    "    2. Performs PGD attack only on those samples\n",
    "    3. Reports attack success rate and robust accuracy on clean-correct set\n",
    "    \n",
    "    Args:\n",
    "        model: Classifier model\n",
    "        loader: DataLoader for evaluation\n",
    "        device: torch device\n",
    "        epsilon: Maximum perturbation budget (L-inf norm)\n",
    "        step_size: Step size for each PGD iteration\n",
    "        num_steps: Number of PGD iterations\n",
    "        batch_indices: Optional indices of batches to evaluate\n",
    "        verbose: Whether to show progress bar\n",
    "    \n",
    "    Returns:\n",
    "        (clean_acc, robust_acc, attack_success_rate): \n",
    "            - clean_acc: Accuracy on all samples\n",
    "            - robust_acc: Accuracy on clean-correct samples after attack\n",
    "            - attack_success_rate: Percentage of clean-correct samples misclassified by attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_samples = 0\n",
    "    clean_correct_count = 0\n",
    "    robust_correct_count = 0\n",
    "    \n",
    "    iterator = enumerate(loader)\n",
    "    if verbose:\n",
    "        iterator = tqdm(iterator, total=len(loader), desc='PGD Attack Evaluation')\n",
    "    \n",
    "    for i, batch in iterator:\n",
    "        if (batch_indices is not None) and (i not in batch_indices):\n",
    "            continue\n",
    "        \n",
    "        # Handle both 2-tuple and 3-tuple batch formats\n",
    "        if len(batch) == 3:\n",
    "            x, y, _ = batch\n",
    "        else:\n",
    "            x, y = batch\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        B = x.size(0)\n",
    "        total_samples += B\n",
    "        \n",
    "        # Step 1: Identify clean-correct samples\n",
    "        with torch.no_grad():\n",
    "            clean_logits = model(x)\n",
    "            clean_pred = clean_logits.argmax(1)\n",
    "            clean_correct_mask = (clean_pred == y)\n",
    "        \n",
    "        clean_correct_count += clean_correct_mask.sum().item()\n",
    "        \n",
    "        # Skip if no correct predictions in this batch\n",
    "        if clean_correct_mask.sum().item() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Select only clean-correct samples\n",
    "        x_correct = x[clean_correct_mask]\n",
    "        y_correct = y[clean_correct_mask]\n",
    "        \n",
    "        # Step 3: Perform PGD attack only on clean-correct samples\n",
    "        x_adv = pgd_attack(model, x_correct, y_correct, epsilon=epsilon, \n",
    "                          step_size=step_size, num_steps=num_steps, random_init=True)\n",
    "        \n",
    "        # Step 4: Evaluate robustness on attacked samples\n",
    "        with torch.no_grad():\n",
    "            adv_logits = model(x_adv)\n",
    "            adv_pred = adv_logits.argmax(1)\n",
    "            robust_correct = (adv_pred == y_correct).sum().item()\n",
    "            robust_correct_count += robust_correct\n",
    "    \n",
    "    # Compute metrics\n",
    "    clean_acc = 100.0 * clean_correct_count / total_samples\n",
    "    robust_acc = 100.0 * robust_correct_count / max(1, clean_correct_count)\n",
    "    attack_success_rate = 100.0 - robust_acc\n",
    "    \n",
    "    print(f\"\\n[PGD Attack Results]\")\n",
    "    print(f\"  Epsilon: {epsilon:.4f} ({epsilon*255:.1f}/255)\")\n",
    "    print(f\"  Step size: {step_size:.4f} ({step_size*255:.1f}/255)\")\n",
    "    print(f\"  Num steps: {num_steps}\")\n",
    "    print(f\"  \" + \"=\"*60)\n",
    "    print(f\"  Total samples: {total_samples}\")\n",
    "    print(f\"  Clean-correct samples: {clean_correct_count} ({clean_acc:.2f}%)\")\n",
    "    print(f\"  \" + \"=\"*60)\n",
    "    print(f\"  Robust accuracy (on clean-correct): {robust_acc:.2f}%\")\n",
    "    print(f\"  Attack success rate: {attack_success_rate:.2f}%\")\n",
    "    print(f\"  \" + \"=\"*60)\n",
    "    \n",
    "    return clean_acc, robust_acc, attack_success_rate\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CW (Carlini-Wagner) Attack\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def cw_loss(logits, targets, num_classes, margin=2.0, targeted=False):\n",
    "    \"\"\"\n",
    "    Carlini-Wagner loss function.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs [B, num_classes]\n",
    "        targets: True labels [B]\n",
    "        num_classes: Number of classes\n",
    "        margin: Confidence margin (kappa in the CW paper)\n",
    "        targeted: Whether this is a targeted attack\n",
    "    \n",
    "    Returns:\n",
    "        loss: CW loss (scalar)\n",
    "    \"\"\"\n",
    "    onehot_targets = F.one_hot(targets, num_classes).float().to(logits.device)\n",
    "    \n",
    "    # Score of the correct class\n",
    "    self_loss = torch.sum(onehot_targets * logits, dim=1)\n",
    "    \n",
    "    # Maximum score among other classes\n",
    "    other_loss = torch.max(\n",
    "        (1 - onehot_targets) * logits - onehot_targets * 10000, dim=1\n",
    "    )[0]\n",
    "    \n",
    "    # CW loss: make correct class score lower than max of other classes\n",
    "    if targeted:\n",
    "        loss = torch.sum(torch.clamp(other_loss - self_loss + margin, min=0))\n",
    "    else:\n",
    "        loss = -torch.sum(torch.clamp(self_loss - other_loss + margin, min=0))\n",
    "    \n",
    "    # Average over batch\n",
    "    loss = loss / logits.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def cw_attack(model, x, y, num_classes, epsilon=8/255, step_size=2/255, num_steps=10, \n",
    "              margin=2.0, random_init=True):\n",
    "    \"\"\"\n",
    "    Perform CW (Carlini-Wagner) attack on input images.\n",
    "    \n",
    "    Args:\n",
    "        model: Classifier model\n",
    "        x: Input images [B, C, H, W]\n",
    "        y: True labels [B]\n",
    "        num_classes: Number of classes\n",
    "        epsilon: Maximum perturbation budget (L-inf norm)\n",
    "        step_size: Step size for each iteration\n",
    "        num_steps: Number of iterations\n",
    "        margin: Confidence margin (kappa)\n",
    "        random_init: Whether to initialize with random noise\n",
    "    \n",
    "    Returns:\n",
    "        x_adv: Adversarial examples [B, C, H, W]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x_adv = x.detach().clone()\n",
    "    \n",
    "    # Random initialization within epsilon ball\n",
    "    if random_init:\n",
    "        x_adv = x_adv + torch.empty_like(x_adv).uniform_(-epsilon, epsilon)\n",
    "        x_adv = torch.clamp(x_adv, min=0, max=1).detach()\n",
    "    \n",
    "    # CW attack iterations\n",
    "    for _ in range(num_steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x_adv)\n",
    "        loss = cw_loss(logits, y, num_classes, margin=margin, targeted=False)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad = torch.autograd.grad(loss, [x_adv])[0]\n",
    "        \n",
    "        # Update adversarial example (gradient ascent to maximize loss)\n",
    "        x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
    "        \n",
    "        # Project back to epsilon ball\n",
    "        delta = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)\n",
    "        x_adv = torch.clamp(x + delta, min=0, max=1).detach()\n",
    "    \n",
    "    return x_adv\n",
    "\n",
    "\n",
    "def evaluate_cw(model, loader, device, num_classes, epsilon=8/255, step_size=2/255, \n",
    "                num_steps=10, margin=2.0, batch_indices=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate model robustness under CW attack on clean-correct samples.\n",
    "    \n",
    "    This function:\n",
    "    1. Identifies correctly classified samples (clean-correct set)\n",
    "    2. Performs CW attack only on those samples\n",
    "    3. Reports attack success rate and robust accuracy on clean-correct set\n",
    "    \n",
    "    Args:\n",
    "        model: Classifier model\n",
    "        loader: DataLoader for evaluation\n",
    "        device: torch device\n",
    "        num_classes: Number of classes\n",
    "        epsilon: Maximum perturbation budget (L-inf norm)\n",
    "        step_size: Step size for each iteration\n",
    "        num_steps: Number of iterations\n",
    "        margin: Confidence margin (kappa)\n",
    "        batch_indices: Optional indices of batches to evaluate\n",
    "        verbose: Whether to show progress bar\n",
    "    \n",
    "    Returns:\n",
    "        (clean_acc, robust_acc, attack_success_rate): \n",
    "            - clean_acc: Accuracy on all samples\n",
    "            - robust_acc: Accuracy on clean-correct samples after attack\n",
    "            - attack_success_rate: Percentage of clean-correct samples misclassified by attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_samples = 0\n",
    "    clean_correct_count = 0\n",
    "    robust_correct_count = 0\n",
    "    \n",
    "    iterator = enumerate(loader)\n",
    "    if verbose:\n",
    "        iterator = tqdm(iterator, total=len(loader), desc='CW Attack Evaluation')\n",
    "    \n",
    "    for i, batch in iterator:\n",
    "        if (batch_indices is not None) and (i not in batch_indices):\n",
    "            continue\n",
    "        \n",
    "        # Handle both 2-tuple and 3-tuple batch formats\n",
    "        if len(batch) == 3:\n",
    "            x, y, _ = batch\n",
    "        else:\n",
    "            x, y = batch\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        B = x.size(0)\n",
    "        total_samples += B\n",
    "        \n",
    "        # Step 1: Identify clean-correct samples\n",
    "        with torch.no_grad():\n",
    "            clean_logits = model(x)\n",
    "            clean_pred = clean_logits.argmax(1)\n",
    "            clean_correct_mask = (clean_pred == y)\n",
    "        \n",
    "        clean_correct_count += clean_correct_mask.sum().item()\n",
    "        \n",
    "        # Skip if no correct predictions in this batch\n",
    "        if clean_correct_mask.sum().item() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Select only clean-correct samples\n",
    "        x_correct = x[clean_correct_mask]\n",
    "        y_correct = y[clean_correct_mask]\n",
    "        \n",
    "        # Step 3: Perform CW attack only on clean-correct samples\n",
    "        x_adv = cw_attack(model, x_correct, y_correct, num_classes, epsilon=epsilon, \n",
    "                         step_size=step_size, num_steps=num_steps, \n",
    "                         margin=margin, random_init=True)\n",
    "        \n",
    "        # Step 4: Evaluate robustness on attacked samples\n",
    "        with torch.no_grad():\n",
    "            adv_logits = model(x_adv)\n",
    "            adv_pred = adv_logits.argmax(1)\n",
    "            robust_correct = (adv_pred == y_correct).sum().item()\n",
    "            robust_correct_count += robust_correct\n",
    "    \n",
    "    # Compute metrics\n",
    "    clean_acc = 100.0 * clean_correct_count / total_samples\n",
    "    robust_acc = 100.0 * robust_correct_count / max(1, clean_correct_count)\n",
    "    attack_success_rate = 100.0 - robust_acc\n",
    "    \n",
    "    print(f\"\\n[CW Attack Results]\")\n",
    "    print(f\"  Epsilon: {epsilon:.4f} ({epsilon*255:.1f}/255)\")\n",
    "    print(f\"  Step size: {step_size:.4f} ({step_size*255:.1f}/255)\")\n",
    "    print(f\"  Num steps: {num_steps}\")\n",
    "    print(f\"  Margin (kappa): {margin}\")\n",
    "    print(f\"  \" + \"=\"*60)\n",
    "    print(f\"  Total samples: {total_samples}\")\n",
    "    print(f\"  Clean-correct samples: {clean_correct_count} ({clean_acc:.2f}%)\")\n",
    "    print(f\"  \" + \"=\"*60)\n",
    "    print(f\"  Robust accuracy (on clean-correct): {robust_acc:.2f}%\")\n",
    "    print(f\"  Attack success rate: {attack_success_rate:.2f}%\")\n",
    "    print(f\"  \" + \"=\"*60)\n",
    "    \n",
    "    return clean_acc, robust_acc, attack_success_rate\n",
    "\n",
    "\n",
    "print(\"Attack functions defined successfully!\")\n",
    "print(\"  - pgd_attack(): Generate PGD adversarial examples\")\n",
    "print(\"  - evaluate_pgd(): Evaluate model under PGD attack (on clean-correct)\")\n",
    "print(\"  - cw_attack(): Generate CW adversarial examples\")\n",
    "print(\"  - evaluate_cw(): Evaluate model under CW attack (on clean-correct)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nfdpxy8kip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example Usage: PGD and CW Attacks\n",
    "# ============================================================================\n",
    "# Uncomment and run to evaluate attacks\n",
    "\n",
    "# # Assuming model and loader are already defined from previous cells\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# DATASET = \"cifar10\"  # or \"tinyimagenet\"\n",
    "# \n",
    "# # Load test dataset\n",
    "# test_dataset, num_classes, out_shape = get_dataset(DATASET, \"./dataset\", train=False, resize=False)\n",
    "# \n",
    "# # Create DataLoader\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=128,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "# \n",
    "# # Model should be loaded from previous cells\n",
    "# # model = ...  # Your trained classifier\n",
    "# \n",
    "# # Attack parameters\n",
    "# EPSILON = 8 / 255  # 8/255 for typical attacks, or 16/255 to match GMM\n",
    "# STEP_SIZE = 2 / 255  # Step size for iterative attacks\n",
    "# NUM_STEPS = 20  # Number of attack iterations\n",
    "# BATCH_INDICES = range(100)  # Evaluate first 100 batches, or None for all\n",
    "# \n",
    "# print(\"=\"*80)\n",
    "# print(\"Evaluating PGD Attack on Clean-Correct Samples\")\n",
    "# print(\"=\"*80)\n",
    "# \n",
    "# # Evaluate PGD attack (only attacks clean-correct samples)\n",
    "# clean_acc_pgd, robust_acc_pgd, attack_success_pgd = evaluate_pgd(\n",
    "#     model=model,\n",
    "#     loader=test_loader,\n",
    "#     device=DEVICE,\n",
    "#     epsilon=EPSILON,\n",
    "#     step_size=STEP_SIZE,\n",
    "#     num_steps=NUM_STEPS,\n",
    "#     batch_indices=BATCH_INDICES,\n",
    "#     verbose=True\n",
    "# )\n",
    "# \n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Evaluating CW Attack on Clean-Correct Samples\")\n",
    "# print(\"=\"*80)\n",
    "# \n",
    "# # Evaluate CW attack (only attacks clean-correct samples)\n",
    "# clean_acc_cw, robust_acc_cw, attack_success_cw = evaluate_cw(\n",
    "#     model=model,\n",
    "#     loader=test_loader,\n",
    "#     device=DEVICE,\n",
    "#     num_classes=num_classes,\n",
    "#     epsilon=EPSILON,\n",
    "#     step_size=STEP_SIZE,\n",
    "#     num_steps=NUM_STEPS,\n",
    "#     margin=2.0,  # CW margin parameter (kappa)\n",
    "#     batch_indices=BATCH_INDICES,\n",
    "#     verbose=True\n",
    "# )\n",
    "# \n",
    "# # Compare results\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Attack Comparison Summary (on Clean-Correct Set)\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"Dataset: {DATASET}\")\n",
    "# print(f\"Epsilon: {EPSILON:.4f} ({EPSILON*255:.1f}/255)\")\n",
    "# print(f\"Num Steps: {NUM_STEPS}\")\n",
    "# print(f\"\\nClean Accuracy: {clean_acc_pgd:.2f}%\")\n",
    "# print(f\"\\nRobust Accuracy:\")\n",
    "# print(f\"  PGD:  {robust_acc_pgd:.2f}%\")\n",
    "# print(f\"  CW:   {robust_acc_cw:.2f}%\")\n",
    "# print(f\"\\nAttack Success Rate:\")\n",
    "# print(f\"  PGD:  {attack_success_pgd:.2f}%\")\n",
    "# print(f\"  CW:   {attack_success_cw:.2f}%\")\n",
    "# \n",
    "# # You can also generate individual adversarial examples\n",
    "# # Example: Generate PGD adversarial examples for a single batch\n",
    "# # x, y, _ = next(iter(test_loader))\n",
    "# # x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "# # \n",
    "# # # Only attack clean-correct samples\n",
    "# # with torch.no_grad():\n",
    "# #     clean_pred = model(x).argmax(1)\n",
    "# #     correct_mask = (clean_pred == y)\n",
    "# # \n",
    "# # x_correct = x[correct_mask]\n",
    "# # y_correct = y[correct_mask]\n",
    "# # \n",
    "# # x_adv_pgd = pgd_attack(model, x_correct, y_correct, epsilon=EPSILON, step_size=STEP_SIZE, num_steps=NUM_STEPS)\n",
    "# # x_adv_cw = cw_attack(model, x_correct, y_correct, num_classes=num_classes, epsilon=EPSILON, step_size=STEP_SIZE, num_steps=NUM_STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
