{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM4PR Analysis - Step 1: Load GMM\n",
    "This notebook loads trained GMM models from fit_gmm2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset: cifar10, Classes: 10, Shape: (3, 224, 224)\n",
      "Using device: cuda\n",
      "\n",
      "Loading classifier: resnet18\n",
      "[check] model params: 62, feat_extractor params: 60\n",
      "[check] They share 60 parameters.\n",
      "Evaluating clean accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2556197/2110917319.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(cfg.clf_ckpt, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clf] accuracy=95.76%\n",
      "\n",
      "Building decoder: bicubic_trainable\n",
      "[Decoder 'bicubic'] 37,779 params\n",
      "✓ Decoder bicubic_trainable built successfully!\n",
      "Loading from: ./ckp/gmm_ckp/gmm_test_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang3_z@WMGDS.WMG.WARWICK.AC.UK/WSPR/utils/gmm4pr.py:985: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Params] Shared trunk: 263,680 | pi: 3,591 | mu: 919,296, | cov: 60,017,920 | Total: 60,940,807\n",
      "Model loaded from ./ckp/gmm_ckp/gmm_test_1.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from config import get_config\n",
    "from utils import GMM4PR, get_dataset, build_model, build_decoder_from_flag, eval_acc\n",
    "\n",
    "# Specify which trained GMM to load\n",
    "CONFIG_NAME = \"test_1\"  # Must match the config name used in fit_gmm2.py\n",
    "CHECKPOINT_DIR = \"./ckp/gmm_ckp/\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load configuration\n",
    "cfg = get_config(CONFIG_NAME)\n",
    "checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"gmm_{cfg.exp_name}.pt\")\n",
    "\n",
    "# Load dataset to get out_shape\n",
    "dataset, num_classes, out_shape = get_dataset(cfg.dataset, cfg.data_root, train=True, resize=True)\n",
    "print(f\"Dataset: {cfg.dataset}, Classes: {num_classes}, Shape: {out_shape}\")\n",
    "\n",
    "# load classifier weights \n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"\\nLoading classifier: {cfg.arch}\")\n",
    "model, feat_extractor = build_model(cfg.arch, num_classes, DEVICE)\n",
    "\n",
    "if not os.path.isfile(cfg.clf_ckpt):\n",
    "    raise FileNotFoundError(f\"Classifier not found: {cfg.clf_ckpt}\")\n",
    "\n",
    "state = torch.load(cfg.clf_ckpt, map_location=\"cpu\")\n",
    "state = state.get(\"state_dict\", state.get(\"model_state\", state))\n",
    "state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
    "\n",
    "model.load_state_dict(state, strict=False)\n",
    "model = model.to(DEVICE).eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "feat_extractor = feat_extractor.to(DEVICE).eval()\n",
    "for p in feat_extractor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Check parameter sharing\n",
    "model_params = {id(p) for p in model.parameters()}\n",
    "feat_params  = {id(p) for p in feat_extractor.parameters()}\n",
    "shared = model_params & feat_params\n",
    "\n",
    "print(f\"[check] model params: {len(model_params)}, feat_extractor params: {len(feat_params)}\")\n",
    "if shared:\n",
    "    print(f\"[check] They share {len(shared)} parameters.\")\n",
    "else:\n",
    "    print(\"[check] No shared parameters.\")\n",
    "\n",
    "print(\"Evaluating clean accuracy...\")\n",
    "eval_acc(model, dataset, DEVICE) # accuracy on training set\n",
    "\n",
    "# Build decoder/up_sampler (if used during training)\n",
    "up_sampler = None\n",
    "if cfg.use_decoder:\n",
    "    print(f\"\\nBuilding decoder: {cfg.decoder_backend}\")\n",
    "    up_sampler = build_decoder_from_flag(\n",
    "        cfg.decoder_backend,\n",
    "        cfg.latent_dim,\n",
    "        out_shape,\n",
    "        DEVICE\n",
    "    )\n",
    "    print(f\"✓ Decoder {cfg.decoder_backend} built successfully!\")\n",
    "else:\n",
    "    print(f\"\\nNo decoder used (use_decoder={cfg.use_decoder})\")\n",
    "\n",
    "# Load the trained GMM model\n",
    "print(f\"Loading from: {checkpoint_path}\")\n",
    "gmm = GMM4PR.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    feat_extractor=feat_extractor, \n",
    "    up_sampler=up_sampler,      \n",
    "    map_location=DEVICE,\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "gmm = gmm.to(DEVICE).eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Probabilistic Robustness (PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PR on clean-correct samples...\n",
      "  GMM: test_1\n",
      "  Samples per image: 100\n",
      "  Batch indices: range(0, 50)\n",
      "[PR@clean] used=309 / seen=320 (clean acc=96.56%), num_samples=500, chunk_size=32 → PR=0.9594 [method: hard (categorical)]\n",
      "\n",
      "============================================================\n",
      "Results:\n",
      "  Clean Accuracy: 96.56%\n",
      "  Probabilistic Robustness (PR): 0.9594\n",
      "  Clean-correct samples used: 309\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_pr_on_clean_correct\n",
    "\n",
    "# Create DataLoader for evaluation\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Set number of samples per image\n",
    "S_SAMPLES = 100  # Number of Monte Carlo samples per image\n",
    "\n",
    "# Optional: Specify batch indices to evaluate (None = all batches)\n",
    "# BATCH_INDICES = None  # Evaluate all batches\n",
    "# BATCH_INDICES = [0, 1, 2, 3, 4]  # Evaluate only first 5 batches\n",
    "BATCH_INDICES = range(50)  # Evaluate batches 100 to 199\n",
    "\n",
    "print(f\"\\nComputing PR on clean-correct samples...\")\n",
    "print(f\"  GMM: {cfg.exp_name}\")\n",
    "print(f\"  Samples per image: {S_SAMPLES}\")\n",
    "print(f\"  Batch indices: {'All' if BATCH_INDICES is None else BATCH_INDICES}\")\n",
    "\n",
    "# Compute PR\n",
    "pr, n_used, clean_acc = compute_pr_on_clean_correct(\n",
    "    model=model,\n",
    "    gmm=gmm,\n",
    "    loader=loader,\n",
    "    out_shape=out_shape,\n",
    "    device=DEVICE,\n",
    "    num_samples=500,\n",
    "    batch_indices=range(10),\n",
    "    temperature=1,  # Very low temperature should approximate hard sampling\n",
    "    use_soft_sampling=False,\n",
    "    chunk_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Results:\")\n",
    "print(f\"  Clean Accuracy: {clean_acc*100:.2f}%\")\n",
    "print(f\"  Probabilistic Robustness (PR): {pr:.4f}\")\n",
    "print(f\"  Clean-correct samples used: {n_used}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
